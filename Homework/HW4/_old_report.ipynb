{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBgThS8q8k3"
      },
      "source": [
        "Full name: Vũ Minh Phát\n",
        "\n",
        "Student ID: 21127739"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdrvDrCrnqz"
      },
      "source": [
        "# HW4: Parallel Radix Sort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXB0wA7yhq9"
      },
      "source": [
        "**To compile your file, you can use this command:** \\\n",
        "`nvcc tên-file.cu -o tên-file-chạy` \\\n",
        "***You can use Vietnamese to anwser the questions***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT_vpPt7I0XJ"
      },
      "source": [
        "**Đề bài**: Song song hóa Radix Sort, trong đó bước scan viết trong một hàm kernel duy nhất (xem file slide \"`CSC14120-Lecture09-RadixSort.pdf`\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX9bsNpxI0XJ"
      },
      "source": [
        "Biên dịch file code `HW4.cu` bằng trình biên dịch `nvcc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tbFLx1i4JxIE"
      },
      "outputs": [],
      "source": [
        "!nvcc HW4.cu -o HW4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qelCzgI0XK"
      },
      "source": [
        "Tiếp theo, ta sẽ chạy chương trình với các kích thước block khác nhau để tìm ra kích thước block tối ưu nhất. Với mỗi kích thước block, ta sẽ chạy file `HW4` theo cả 2 cách:\n",
        "- Cách 1: Chạy file `HW4` một cách thông thường. Sử dụng câu lệnh: `./HW4 <block_size>`\n",
        "- Cách 2: Dùng thêm `nvprof` để xem chi tiết thực thi của chương trình. Sử dụng câu lệnh: `nvprof ./HW4 <block_size>`\n",
        "\n",
        "Ta sẽ dùng kết quả của **cách 1** để tính ra thời gian thực thi của chương trình ứng với mỗi kích thước block. Do đó, ta sẽ chạy code cell này nhiều lần để thu được kết quả chính xác nhất. Còn đối với **cách 2**, ta sẽ quan tâm đến thời gian thực thi các công việc trong phần \"GPU activities\". Thông tin này sẽ giúp ta hiểu rõ hơn về tác động của kích thước block đến hiệu suất của chương trình.\n",
        "\n",
        "Bây giờ, ta sẽ bắt đầu chạy file `HW4` với các kích thước block khác nhau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkWJD6uhI0XL"
      },
      "source": [
        "- Với kích thước block là `256`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZNqZuECjNso",
        "outputId": "153154ac-e285-4105-e94d-8bb53601bb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9760.913 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 256, Grid size: 32769\n",
            "Time: 915.423 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-xlQLAeK8OY",
        "outputId": "49036a8b-7a97-4314-cc05-dec2acac0e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==564== NVPROF is profiling process 564, command: ./HW4 256\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9411.372 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 256, Grid size: 32769\n",
            "Time: 918.616 ms\n",
            "CORRECT :)\n",
            "==564== Profiling application: ./HW4 256\n",
            "==564== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   87.14%  791.14ms        32  24.723ms  20.240ms  47.675ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                    5.41%  49.119ms         1  49.119ms  49.119ms  49.119ms  [CUDA memcpy DtoH]\n",
            "                    3.95%  35.852ms        32  1.1204ms  1.0462ms  1.1295ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    1.92%  17.477ms        32  546.16us  535.65us  557.34us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    1.58%  14.335ms        65  220.54us     351ns  14.307ms  [CUDA memcpy HtoD]\n",
            "      API calls:   75.46%  844.91ms        96  8.8012ms  537.82us  47.690ms  cudaDeviceSynchronize\n",
            "                   17.94%  200.90ms         4  50.224ms  1.2920us  200.89ms  cudaEventCreate\n",
            "                    5.76%  64.443ms         2  32.222ms  14.515ms  49.928ms  cudaMemcpy\n",
            "                    0.43%  4.7700ms         5  953.99us  280.67us  1.6397ms  cudaFree\n",
            "                    0.15%  1.7170ms        96  17.884us  4.5650us  541.85us  cudaLaunchKernel\n",
            "                    0.15%  1.6741ms        64  26.158us  5.0410us  973.53us  cudaMemcpyToSymbol\n",
            "                    0.07%  807.63us         5  161.53us  75.166us  301.56us  cudaMalloc\n",
            "                    0.01%  130.11us       114  1.1410us     144ns  51.529us  cuDeviceGetAttribute\n",
            "                    0.01%  105.39us         1  105.39us  105.39us  105.39us  cudaGetDeviceProperties\n",
            "                    0.01%  74.170us         4  18.542us  3.7450us  44.034us  cudaEventRecord\n",
            "                    0.00%  35.938us         4  8.9840us  5.9680us  16.670us  cudaEventSynchronize\n",
            "                    0.00%  28.337us        96     295ns     135ns     994ns  cudaGetLastError\n",
            "                    0.00%  12.802us         1  12.802us  12.802us  12.802us  cuDeviceGetName\n",
            "                    0.00%  6.5410us         4  1.6350us     739ns  2.8010us  cudaEventDestroy\n",
            "                    0.00%  5.6740us         2  2.8370us  1.9110us  3.7630us  cudaEventElapsedTime\n",
            "                    0.00%  5.0500us         1  5.0500us  5.0500us  5.0500us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.1440us         1  4.1440us  4.1440us  4.1440us  cuDeviceTotalMem\n",
            "                    0.00%  1.3750us         3     458ns     197ns     904ns  cuDeviceGetCount\n",
            "                    0.00%  1.0520us         2     526ns     189ns     863ns  cuDeviceGet\n",
            "                    0.00%     472ns         1     472ns     472ns     472ns  cuModuleGetLoadingMode\n",
            "                    0.00%     229ns         1     229ns     229ns     229ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYnr_fmI0XL"
      },
      "source": [
        "- Với kích thước block là `512`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVFUj14OYUyy",
        "outputId": "90f95862-cfb1-4e16-9103-6dc4593b4dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9446.203 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 512, Grid size: 16385\n",
            "Time: 573.924 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKM5xF3aLBxf",
        "outputId": "929dc228-342e-4d31-c8dc-b6d0508a70c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==783== NVPROF is profiling process 783, command: ./HW4 512\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9470.033 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 512, Grid size: 16385\n",
            "Time: 570.200 ms\n",
            "CORRECT :)\n",
            "==783== Profiling application: ./HW4 512\n",
            "==783== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   79.47%  445.67ms        32  13.927ms  10.120ms  23.764ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                    8.19%  45.929ms         1  45.929ms  45.929ms  45.929ms  [CUDA memcpy DtoH]\n",
            "                    6.37%  35.735ms        32  1.1167ms  1.0344ms  1.1234ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    3.19%  17.870ms        32  558.44us  542.97us  601.79us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    2.78%  15.594ms        65  239.90us     351ns  15.564ms  [CUDA memcpy HtoD]\n",
            "      API calls:   66.71%  499.70ms        96  5.2052ms  545.64us  23.772ms  cudaDeviceSynchronize\n",
            "                   23.86%  178.71ms         4  44.677ms  1.4110us  178.70ms  cudaEventCreate\n",
            "                    8.33%  62.426ms         2  31.213ms  15.763ms  46.663ms  cudaMemcpy\n",
            "                    0.62%  4.6256ms         5  925.12us  264.71us  1.1235ms  cudaFree\n",
            "                    0.20%  1.4879ms        96  15.498us  4.8270us  607.41us  cudaLaunchKernel\n",
            "                    0.15%  1.1547ms         5  230.94us  112.96us  380.37us  cudaMalloc\n",
            "                    0.07%  546.07us        64  8.5320us  4.9500us  28.567us  cudaMemcpyToSymbol\n",
            "                    0.02%  126.22us       114  1.1070us     135ns  50.332us  cuDeviceGetAttribute\n",
            "                    0.01%  86.422us         1  86.422us  86.422us  86.422us  cudaGetDeviceProperties\n",
            "                    0.01%  60.223us         4  15.055us  3.5590us  30.203us  cudaEventRecord\n",
            "                    0.00%  30.765us         4  7.6910us  6.3510us  11.167us  cudaEventSynchronize\n",
            "                    0.00%  23.835us        96     248ns     131ns     593ns  cudaGetLastError\n",
            "                    0.00%  11.803us         1  11.803us  11.803us  11.803us  cuDeviceGetName\n",
            "                    0.00%  9.1460us         4  2.2860us     786ns  3.7510us  cudaEventDestroy\n",
            "                    0.00%  6.3090us         2  3.1540us  2.2820us  4.0270us  cudaEventElapsedTime\n",
            "                    0.00%  5.1970us         1  5.1970us  5.1970us  5.1970us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.0080us         1  5.0080us  5.0080us  5.0080us  cuDeviceTotalMem\n",
            "                    0.00%  1.3490us         3     449ns     213ns     847ns  cuDeviceGetCount\n",
            "                    0.00%  1.0050us         2     502ns     160ns     845ns  cuDeviceGet\n",
            "                    0.00%     596ns         1     596ns     596ns     596ns  cuModuleGetLoadingMode\n",
            "                    0.00%     219ns         1     219ns     219ns     219ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebMjR45-0Io"
      },
      "source": [
        "- Với kích thước block là `1024`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2wu_ELCI0XM",
        "outputId": "208f5b36-50ea-4249-ec8c-0940e3cd59f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9453.724 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 1024, Grid size: 8193\n",
            "Time: 428.116 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Mnq98KLKVu",
        "outputId": "ddf7cd25-4ee4-46ee-a1d0-238a35fcd778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==1205== NVPROF is profiling process 1205, command: ./HW4 1024\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9529.373 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 1024, Grid size: 8193\n",
            "Time: 399.449 ms\n",
            "CORRECT :)\n",
            "==1205== Profiling application: ./HW4 1024\n",
            "==1205== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   69.62%  271.69ms        32  8.4903ms  5.5262ms  11.982ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                   12.26%  47.836ms         1  47.836ms  47.836ms  47.836ms  [CUDA memcpy DtoH]\n",
            "                    9.31%  36.318ms        32  1.1349ms  1.0386ms  1.1432ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    4.80%  18.742ms        32  585.68us  545.76us  633.02us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    4.01%  15.663ms        65  240.97us     384ns  15.630ms  [CUDA memcpy HtoD]\n",
            "      API calls:   57.54%  327.45ms        96  3.4110ms  543.10us  11.989ms  cudaDeviceSynchronize\n",
            "                   29.78%  169.47ms         4  42.367ms     936ns  169.46ms  cudaEventCreate\n",
            "                   11.32%  64.394ms         2  32.197ms  15.805ms  48.589ms  cudaMemcpy\n",
            "                    0.80%  4.5796ms         5  915.91us  255.67us  1.1091ms  cudaFree\n",
            "                    0.25%  1.4088ms        96  14.674us  4.4080us  591.31us  cudaLaunchKernel\n",
            "                    0.15%  845.51us         5  169.10us  98.400us  314.91us  cudaMalloc\n",
            "                    0.09%  535.98us        64  8.3740us  4.7740us  33.610us  cudaMemcpyToSymbol\n",
            "                    0.02%  125.42us       114  1.1000us     137ns  50.035us  cuDeviceGetAttribute\n",
            "                    0.02%  85.463us         1  85.463us  85.463us  85.463us  cudaGetDeviceProperties\n",
            "                    0.01%  51.636us         4  12.909us  3.0830us  26.633us  cudaEventRecord\n",
            "                    0.00%  27.013us         4  6.7530us  5.1870us  9.9740us  cudaEventSynchronize\n",
            "                    0.00%  24.223us        96     252ns     135ns     970ns  cudaGetLastError\n",
            "                    0.00%  11.233us         1  11.233us  11.233us  11.233us  cuDeviceGetName\n",
            "                    0.00%  7.4250us         4  1.8560us     868ns  3.2720us  cudaEventDestroy\n",
            "                    0.00%  5.9690us         1  5.9690us  5.9690us  5.9690us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.9710us         2  2.4850us  2.2790us  2.6920us  cudaEventElapsedTime\n",
            "                    0.00%  4.9300us         1  4.9300us  4.9300us  4.9300us  cuDeviceTotalMem\n",
            "                    0.00%  1.8760us         3     625ns     238ns  1.3780us  cuDeviceGetCount\n",
            "                    0.00%  1.0310us         2     515ns     211ns     820ns  cuDeviceGet\n",
            "                    0.00%     375ns         1     375ns     375ns     375ns  cuModuleGetLoadingMode\n",
            "                    0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Câu hỏi**: Giải thích tại sao khi thay đổi kích thước block thì kết quả lại thay đổi như vậy?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JDLuQCYcDZx"
      },
      "source": [
        "**`1.` Quan sát sự thay đổi của kết quả khi thay đổi kích thước block và đưa ra một số nhận xét:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jf10kTHI0XM"
      },
      "source": [
        "Đầu tiên, ta sẽ tạo ra một bảng thống kê thời gian chạy của chương trình ứng với các kích thước block khác nhau. Với mỗi kích thước block, ta chạy chương trình `5 lần`, sau đó tính ra thời gian chạy trung bình. Trước khi bắt đầu ghi nhận thời gian, ta sẽ chạy chương trình một lần để \"làm nóng\" GPU.\n",
        "\n",
        "| Kích thước block (Block size) | Số lượng block (Grid size) | Chạy lần 1 (ms) | Chạy lần 2 (ms) | Chạy lần 3 (ms) | Chạy lần 4 (ms) | Chạy lần 5 (ms) | Thời gian chạy trung bình (ms) |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 256 | 32769 | 940.082 | 945.507 | 907.579 | 917.287 | 940.38 | 930.167 |\n",
        "| 512 | 16385 | 596.125 | 580.446 | 590.943 | 598.832 | 568.071 | 586.883 |\n",
        "| 1024 | 8193 | 424.264 | 439.849 | 397.475 | 420.168 | 430.14 | **422.379** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Nhận xét**:\n",
        "\n",
        "    - Từ bảng thống kê thời gian chạy của chương trình ứng với các kích thước block khác nhau, ta phát hiện có một xu hướng rõ rệt: *thời gian chạy của chương trình giảm dần khi kích thước block tăng lên*. Điều này được thể hiện rõ qua bảng thống kê, với thời gian chạy trung bình giảm từ 930.167ms (block size 256) xuống còn 422.379ms (block size 1024).\n",
        "\n",
        "    - Ta sẽ tìm hiểu nguyên nhân dẫn đến hiện tượng này bằng cách phân tích chi tiết hơn về các hoạt động của GPU trong khi thực thi chương trình."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBYlqxyycDZy"
      },
      "source": [
        "Quan sát thời gian thực thi các công việc trong phần \"GPU activities\" của `nvprof` ứng với mỗi kích thước block, ta phát hiện rằng thời gian thực thi của các công việc:\n",
        "\n",
        "- Sao chép dữ liệu từ host sang device (HtoD)\n",
        "- Sao chép dữ liệu từ device sang host (DtoH)\n",
        "- Bước rút trích bit\n",
        "- Bước tính rank và ghi kết quả vào mảng dst\n",
        "\n",
        "không thay đổi nhiều khi ta thay đổi kích thước block. Nghĩa là, thời gian thực thi của các công việc này không phụ thuộc quá nhiều vào kích thước block.\n",
        "\n",
        "| Công việc | Thời gian chạy (ms) |\n",
        "| --- | --- |\n",
        "| [CUDA memcpy HtoD] | 15 |\n",
        "| [CUDA memcpy DtoH] | 46 |\n",
        "| extractBitsKernel | 18 |\n",
        "| computeRankAndWriteToDstKernel | 36 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesNN9sfcDZy"
      },
      "source": [
        "Tuy nhiên, thời gian thực thi của bước \"exclusive scan\" để tính ra mảng `nOnesBefore` lại có xu hướng giảm xuống khi ta tăng kích thước block. Và chính sự khác biệt trong thời gian thực thi của hàm kernel này đã ảnh hưởng trực tiếp đến tổng thời gian chạy của chương trình ứng với các kích thước block khác nhau.\n",
        "\n",
        "| Kích thước block (Block size) | Thời gian chạy của bước \"exclusive scan\" (ms) |\n",
        "| --- | --- |\n",
        "| 256 | 809.15 |\n",
        "| 512 | 451.90 |\n",
        "| 1024 | **297.94** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Nhận xét**:\n",
        "\n",
        "    - Sau khi tiến hành phân tích chi tiết hơn về thời gian thực thi của các công việc có sử dụng GPU, ta nhận thấy rằng nguyên nhân chính dẫn đến sự thay đổi thời gian chạy của chương trình khi thay đổi kích thước block nằm ở bước \"exclusive scan\" để tính mảng `nOnesBefore`. Trong khi thời gian thực thi của các công việc khác như sao chép dữ liệu và các bước kernel khác ít thay đổi, thì thời gian thực thi của bước \"exclusive scan\" giảm mạnh khi kích thước block tăng lên (từ 809.15ms xuống 297.94ms).\n",
        "\n",
        "    - Tiếp theo, ta sẽ phân tích chi tiết về mỗi lần lặp của thuật toán Radix Sort, cũng như cách hoạt động của bước \"exclusive scan\" để hiểu rõ hơn về nguyên nhân dẫn đến sự thay đổi này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cE0Fy1cDZy"
      },
      "source": [
        "**`2.` Phân tích chi tiết về mỗi lần lặp của thuật toán Radix Sort và bước \"exclusive scan\":**\n",
        "\n",
        "Trong mỗi lần lặp (iteration) của thuật toán Radix Sort, ta cần phải thực hiện 3 bước chính:\n",
        "\n",
        "- Bước 1: Rút trích bit thứ `bitIdx` từ mỗi phần tử của mảng `src` và ghi vào mảng `bits`.\n",
        "\n",
        "- Bước 2: Dựa vào mảng `bits`, ta tính ra mảng `nOnesBefore` (số lượng bit 1 trước mỗi phần tử của mảng `src`).\n",
        "\n",
        "- Bước 3: Dựa vào mảng `bits` và `nOnesBefore`, ta sắp xếp lại vị trí của các phần tử trong mảng `src` và ghi kết quả vào mảng `dst`. Vị trí mới của mỗi phần tử trong mảng `src` sẽ được tính dựa vào số lượng bit 0 và bit 1 trước nó (xem công thức tính `rank` trong slide).\n",
        "\n",
        "Cả 3 bước này đều có thể thực hiện song song trên GPU thông qua việc sử dụng các hàm kernel. Trong đó, với \"**bước 1**\" và \"**bước 3**\", các thread block có thể hoạt động độc lập với nhau, tức là các block không cần chờ đợi tín hiệu từ các block khác. Do đó, sự thay đổi về số lượng block cần thiết trong grid không ảnh hưởng nhiều đến hiệu suất của chương trình.\n",
        "\n",
        "Ngược lại, ở \"**bước 2**\", ta cần phải thực hiện một **thuật toán \"exclusive scan\"** song song trên toàn bộ grid, tức là kết quả của mỗi block sẽ có sự phụ thuộc vào kết quả của các block \"trước đó\". Thuật toán \"exclusive scan\" này có thể được thực hiện thông qua 4 bước chính:\n",
        "\n",
        "- Bước 2.1: Tính ra block index $bi$ của mỗi block theo thứ tự được đưa vào SM của chúng.\n",
        "\n",
        "- Bước 2.2: Thực hiện scan cục bộ trên mỗi block để tính ra số lượng bit 1 trước mỗi phần tử trong block đó. Đồng thời, ta cũng cần phải lưu lại tổng số lượng bit 1 của mỗi block vào một mảng riêng để sử dụng cho bước tiếp theo.\n",
        "\n",
        "- Bước 2.3: Block $bi$ sẽ phải chờ đến khi block $bi-1$ báo tín hiệu rằng nó đã hoàn thành việc tính tổng toàn cục của $bi$ block trước đó (từ block 0 đến block $bi-1$). Sau đó, block $bi$ sẽ cộng \"tổng toàn cục của các block trước đó\" vào \"tổng cục bộ của mình\" và truyền tín hiệu cho block $bi+1$ biết rằng nó có thể bắt đầu công việc của mình. \n",
        "\n",
        "- Bước 2.4: Thêm tổng toàn cục của $bi$ block trước đó vào kết quả của mỗi phần tử trong block $bi$.\n",
        "\n",
        "Trong 4 bước chính được liệt kê ở trên, thì chỉ có \"**bước 2.2**\" và \"**bước 2.4**\" là được thực hiện song song một cách \"tự nhiên\", tức là hoạt động của mỗi thread block không phụ thuộc vào (hoặc không cần chờ đợi) các block khác. Còn \"**bước 2.1**\" và \"**bước 2.3**\" lại đòi hỏi sự phối hợp giữa các block, tức là các thread block phải làm việc một cách tuần tự, block $bi$ phải chờ đến khi block $bi-1$ bật tín hiệu \"hoàn thành\" thì mới được thực hiện bước tiếp theo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**`3.` Giải thích về ảnh hưởng của kích thước block đến hiệu suất của chương trình:**\n",
        "\n",
        "Lý do cho sự thay đổi về thời gian chạy của chương trình khi thay đổi kích thước block nằm ở cách thuật toán \"exclusive scan\" được thực hiện trên GPU. Như đã phân tích, thuật toán này bao gồm các bước cần sự phối hợp giữa các block, đặc biệt là \"**bước 2.3**\", nơi block $bi$ phải chờ block $bi-1$. **Khi kích thước block tăng lên, thì số lượng block cần thiết để xử lý cùng một lượng dữ liệu giảm xuống**. Điều này dẫn đến:\n",
        "\n",
        "`3.1.` **Giảm số lượng bước đồng bộ hóa giữa các block**:\n",
        "\n",
        "- Khi kích thước block nhỏ, ta sẽ cần nhiều block hơn để xử lý toàn bộ dữ liệu, dẫn đến nhiều lần đồng bộ hóa giữa các block. Điều này gây ra overhead lớn hơn. Khi tăng kích thước block, số lượng block cần thiết để xử lý cùng một khối lượng dữ liệu sẽ giảm, từ đó giảm số lần đồng bộ hóa giữa các block. Đây chính là yếu tố then chốt giúp giảm thời gian thực thi. Việc đồng bộ hóa giữa các block là một hoạt động tốn kém, và việc giảm thiểu nó sẽ cải thiện hiệu suất đáng kể. Điều này giải thích tại sao thời gian thực thi của bước \"exclusive scan\" giảm mạnh khi kích thước block tăng lên. \n",
        "\n",
        "`3.2.` **Tăng hiệu quả sử dụng bộ nhớ chia sẻ (shared memory)**:\n",
        "\n",
        "- Mỗi block sử dụng shared memory để lưu trữ dữ liệu trung gian. Với kích thước block lớn hơn, bộ nhớ chia sẻ có thể được sử dụng hiệu quả hơn vì nhiều dữ liệu có thể được xử lý cục bộ mà không cần truy cập nhiều vào bộ nhớ toàn cục (global memory). Điều này giảm tải băng thông bộ nhớ toàn cục, giúp cải thiện tốc độ truy cập dữ liệu.\n",
        "\n",
        "`3.3.` **Tăng hiệu suất tính toán cục bộ**:\n",
        "\n",
        "- Kích thước block lớn hơn cho phép mỗi block xử lý nhiều dữ liệu hơn ở cục bộ. Điều này có thể tận dụng tốt hơn bộ nhớ shared memory của SM (Streaming Multiprocessor) và giảm thiểu số lần truy cập bộ nhớ global memory, vốn chậm hơn. Bước 2.2 và 2.4 (tính toán cục bộ trong mỗi block) sẽ hiệu quả hơn với block size lớn hơn.\n",
        "\n",
        "**Kết luận**:\n",
        "\n",
        "- Sự giảm thời gian chạy khi tăng kích thước block là do GPU có thể tận dụng tốt hơn khả năng xử lý song song và bộ nhớ chia sẻ, tối ưu hóa việc sử dụng băng thông bộ nhớ.\n",
        "\n",
        "- Đặc biệt, việc tăng kích thước block giúp giảm số lượng block cần thiết, từ đó giảm số lần đồng bộ hóa giữa các block trong bước \"exclusive scan\", dẫn đến giảm đáng kể thời gian thực thi của bước này và do đó giảm tổng thời gian chạy của chương trình.\n",
        "\n",
        "- Điều này cho thấy việc lựa chọn kích thước block phù hợp là rất quan trọng để tối ưu hiệu năng của các thuật toán song song trên GPU, đặc biệt là các thuật toán có bước cần sự phối hợp giữa các block như thuật toán \"exclusive scan\" mà chúng ta đã cài đặt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I7cPRSKcDZy"
      },
      "source": [
        "> References:  \n",
        "> [1] Book \"Programming Massively Parallel Processors: A Hands-on Approach\", chapter \"11 - Prefix sum (scan)\", by David B. Kirk and Wen-mei W. Hwu.  \n",
        "> [2] Book \"Programming Massively Parallel Processors: A Hands-on Approach\", chapter \"13 - Sorting\", by David B. Kirk and Wen-mei W. Hwu.  \n",
        "> [3] Slide bài giảng \"2023-CSC14120-Lecture08-Scan.pdf\" được cung cấp trong môn học.  \n",
        "> [4] Slide bài giảng \"2023-CSC14120-Lecture09-RadixSort.pdf\" được cung cấp trong môn học.  \n",
        "> [5] File code \"08-Scan.cu\" trong phần Demo code trên drive được cung cấp trong môn học.  \n",
        "> [6] \"Chapter 39. Parallel Prefix Sum (Scan) with CUDA\" in \"GPU Gems 3\" by Mark Harris, Shubhabrata Sengupta, John D. Owens - [developer.nvidia.com](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda).  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "graphmining",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
