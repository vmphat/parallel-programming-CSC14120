{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBgThS8q8k3"
      },
      "source": [
        "Full name: Vũ Minh Phát\n",
        "\n",
        "Student ID: 21127739"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdrvDrCrnqz"
      },
      "source": [
        "# HW4: Parallel Radix Sort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXB0wA7yhq9"
      },
      "source": [
        "**To compile your file, you can use this command:** \\\n",
        "`nvcc tên-file.cu -o tên-file-chạy` \\\n",
        "***You can use Vietnamese to anwser the questions***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT_vpPt7I0XJ"
      },
      "source": [
        "**Đề bài**: Song song hóa Radix Sort, trong đó bước scan viết trong một hàm kernel duy nhất (xem file slide \"`CSC14120-Lecture09-RadixSort.pdf`\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX9bsNpxI0XJ"
      },
      "source": [
        "Biên dịch file code `HW4.cu` bằng trình biên dịch `nvcc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tbFLx1i4JxIE"
      },
      "outputs": [],
      "source": [
        "!nvcc HW4.cu -o HW4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qelCzgI0XK"
      },
      "source": [
        "Tiếp theo, ta sẽ chạy chương trình với các kích thước block khác nhau để tìm ra kích thước block tối ưu nhất. Với mỗi kích thước block, ta sẽ chạy file `HW4` theo cả 2 cách:\n",
        "- Cách 1: Chạy file `HW4` một cách thông thường. Sử dụng câu lệnh: `./HW4 <block_size>`\n",
        "- Cách 2: Dùng thêm `nvprof` để xem chi tiết thực thi của chương trình. Sử dụng câu lệnh: `nvprof ./HW4 <block_size>`\n",
        "\n",
        "Ta sẽ dùng kết quả của **cách 1** để tính ra thời gian thực thi của chương trình ứng với mỗi kích thước block. Do đó, ta sẽ chạy code cell này nhiều lần để thu được kết quả chính xác nhất. Còn đối với **cách 2**, ta sẽ quan tâm đến thời gian thực thi các công việc trong phần \"GPU activities\". Thông tin này sẽ giúp ta hiểu rõ hơn về tác động của kích thước block đến hiệu suất của chương trình.\n",
        "\n",
        "Bây giờ, ta sẽ bắt đầu chạy file `HW4` với các kích thước block khác nhau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkWJD6uhI0XL"
      },
      "source": [
        "- Với kích thước block là `256`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZNqZuECjNso",
        "outputId": "9e05f803-aa54-4453-ac99-32b3d0fb5a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10898.235 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 256, Grid size: 32769\n",
            "Time: 910.620 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-xlQLAeK8OY",
        "outputId": "1733202d-89b8-456f-f08a-35433a55f096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==1294== NVPROF is profiling process 1294, command: ./HW4 256\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10471.082 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 256, Grid size: 32769\n",
            "Time: 929.890 ms\n",
            "CORRECT :)\n",
            "==1294== Profiling application: ./HW4 256\n",
            "==1294== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   87.23%  801.77ms        32  25.055ms  20.250ms  47.397ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                    5.37%  49.319ms         1  49.319ms  49.319ms  49.319ms  [CUDA memcpy DtoH]\n",
            "                    3.90%  35.894ms        32  1.1217ms  1.0418ms  1.1292ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    1.91%  17.513ms        32  547.29us  544.40us  558.29us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    1.60%  14.687ms        65  225.95us     351ns  14.659ms  [CUDA memcpy HtoD]\n",
            "      API calls:   83.00%  855.59ms        96  8.9124ms  548.04us  47.404ms  cudaDeviceSynchronize\n",
            "                    9.78%  100.81ms         4  25.202ms  1.0630us  100.80ms  cudaEventCreate\n",
            "                    6.30%  64.892ms         2  32.446ms  14.845ms  50.047ms  cudaMemcpy\n",
            "                    0.45%  4.6449ms         5  928.98us  278.47us  1.1304ms  cudaFree\n",
            "                    0.19%  1.9283ms        64  30.129us  4.9300us  1.2689ms  cudaMemcpyToSymbol\n",
            "                    0.16%  1.6215ms        96  16.890us  5.2570us  526.89us  cudaLaunchKernel\n",
            "                    0.09%  904.94us         5  180.99us  80.998us  331.59us  cudaMalloc\n",
            "                    0.01%  145.69us       114  1.2780us     164ns  57.719us  cuDeviceGetAttribute\n",
            "                    0.01%  110.25us         1  110.25us  110.25us  110.25us  cudaGetDeviceProperties\n",
            "                    0.00%  49.961us         4  12.490us  3.3400us  23.357us  cudaEventRecord\n",
            "                    0.00%  29.667us        96     309ns     141ns     714ns  cudaGetLastError\n",
            "                    0.00%  28.429us         4  7.1070us  5.4830us  9.1290us  cudaEventSynchronize\n",
            "                    0.00%  12.218us         1  12.218us  12.218us  12.218us  cuDeviceGetName\n",
            "                    0.00%  7.2380us         4  1.8090us     804ns  3.0200us  cudaEventDestroy\n",
            "                    0.00%  5.3520us         1  5.3520us  5.3520us  5.3520us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.2180us         1  5.2180us  5.2180us  5.2180us  cuDeviceTotalMem\n",
            "                    0.00%  5.1430us         2  2.5710us  2.3440us  2.7990us  cudaEventElapsedTime\n",
            "                    0.00%  3.2290us         2  1.6140us     207ns  3.0220us  cuDeviceGet\n",
            "                    0.00%  2.0250us         3     675ns     229ns  1.4670us  cuDeviceGetCount\n",
            "                    0.00%     531ns         1     531ns     531ns     531ns  cuModuleGetLoadingMode\n",
            "                    0.00%     307ns         1     307ns     307ns     307ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYnr_fmI0XL"
      },
      "source": [
        "- Với kích thước block là `512`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVFUj14OYUyy",
        "outputId": "7e609472-52bb-4adf-c9e9-7b64cf5e42bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10452.003 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 512, Grid size: 16385\n",
            "Time: 591.643 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKM5xF3aLBxf",
        "outputId": "72d23e8a-677b-420e-80bd-7b3b9d9cd5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==1579== NVPROF is profiling process 1579, command: ./HW4 512\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9986.422 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 512, Grid size: 16385\n",
            "Time: 615.410 ms\n",
            "CORRECT :)\n",
            "==1579== Profiling application: ./HW4 512\n",
            "==1579== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   79.96%  483.80ms        32  15.119ms  10.169ms  23.753ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                    8.28%  50.087ms         1  50.087ms  50.087ms  50.087ms  [CUDA memcpy DtoH]\n",
            "                    5.91%  35.740ms        32  1.1169ms  1.0376ms  1.1239ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    2.98%  18.028ms        32  563.37us  542.96us  602.35us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    2.88%  17.435ms        65  268.23us     352ns  17.404ms  [CUDA memcpy HtoD]\n",
            "      API calls:   72.48%  538.17ms        96  5.6060ms  545.53us  23.929ms  cudaDeviceSynchronize\n",
            "                   17.09%  126.92ms         4  31.730ms  1.5920us  126.91ms  cudaEventCreate\n",
            "                    9.24%  68.581ms         2  34.291ms  17.673ms  50.909ms  cudaMemcpy\n",
            "                    0.62%  4.6398ms         5  927.96us  264.22us  1.1379ms  cudaFree\n",
            "                    0.26%  1.9037ms        96  19.830us  5.2290us  655.38us  cudaLaunchKernel\n",
            "                    0.16%  1.1860ms         5  237.19us  114.69us  413.56us  cudaMalloc\n",
            "                    0.09%  640.35us        64  10.005us  5.1820us  31.446us  cudaMemcpyToSymbol\n",
            "                    0.03%  207.03us       114  1.8160us     247ns  87.070us  cuDeviceGetAttribute\n",
            "                    0.02%  130.32us         1  130.32us  130.32us  130.32us  cudaGetDeviceProperties\n",
            "                    0.01%  64.443us         4  16.110us  4.1670us  30.677us  cudaEventRecord\n",
            "                    0.00%  31.067us         4  7.7660us  5.2490us  12.145us  cudaEventSynchronize\n",
            "                    0.00%  29.418us        96     306ns     160ns     745ns  cudaGetLastError\n",
            "                    0.00%  13.706us         1  13.706us  13.706us  13.706us  cuDeviceGetName\n",
            "                    0.00%  8.2600us         1  8.2600us  8.2600us  8.2600us  cuDeviceGetPCIBusId\n",
            "                    0.00%  8.0120us         4  2.0030us     943ns  3.7400us  cudaEventDestroy\n",
            "                    0.00%  6.8380us         2  3.4190us  2.3790us  4.4590us  cudaEventElapsedTime\n",
            "                    0.00%  6.3550us         1  6.3550us  6.3550us  6.3550us  cuDeviceTotalMem\n",
            "                    0.00%  2.5150us         3     838ns     282ns  1.8580us  cuDeviceGetCount\n",
            "                    0.00%  1.4350us         2     717ns     334ns  1.1010us  cuDeviceGet\n",
            "                    0.00%     619ns         1     619ns     619ns     619ns  cuModuleGetLoadingMode\n",
            "                    0.00%     383ns         1     383ns     383ns     383ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebMjR45-0Io"
      },
      "source": [
        "- Với kích thước block là `1024`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2wu_ELCI0XM",
        "outputId": "b49fc6d1-3c0b-40cb-8025-d2d675194fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10458.073 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 1024, Grid size: 8193\n",
            "Time: 413.851 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW4 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Mnq98KLKVu",
        "outputId": "e7139693-6583-44bc-89af-e3350390ad3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==1814== NVPROF is profiling process 1814, command: ./HW4 1024\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10538.454 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Block size: 1024, Grid size: 8193\n",
            "Time: 445.909 ms\n",
            "CORRECT :)\n",
            "==1814== Profiling application: ./HW4 1024\n",
            "==1814== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   68.37%  293.56ms        32  9.1738ms  5.5575ms  12.001ms  BrentKungExclusiveScanKernel(int*, int*, int, int*)\n",
            "                   15.05%  64.598ms         1  64.598ms  64.598ms  64.598ms  [CUDA memcpy DtoH]\n",
            "                    8.46%  36.327ms        32  1.1352ms  1.0374ms  1.1421ms  computeRankAndWriteToDstKernel(unsigned int*, int*, int*, unsigned int*, int)\n",
            "                    4.44%  19.051ms        32  595.35us  545.14us  634.39us  extractBitsKernel(unsigned int*, int*, int, int)\n",
            "                    3.68%  15.806ms        65  243.18us     384ns  15.771ms  [CUDA memcpy HtoD]\n",
            "      API calls:   65.10%  354.29ms        96  3.6905ms  546.70us  12.544ms  cudaDeviceSynchronize\n",
            "                   18.06%  98.302ms         4  24.575ms  1.1980us  98.293ms  cudaEventCreate\n",
            "                   15.00%  81.613ms         2  40.806ms  15.984ms  65.629ms  cudaMemcpy\n",
            "                    0.88%  4.8045ms         5  960.90us  381.16us  1.2348ms  cudaFree\n",
            "                    0.46%  2.4814ms        96  25.848us  7.9770us  655.96us  cudaLaunchKernel\n",
            "                    0.24%  1.3021ms         5  260.43us  139.65us  412.53us  cudaMalloc\n",
            "                    0.18%  984.40us        64  15.381us  6.1510us  58.547us  cudaMemcpyToSymbol\n",
            "                    0.03%  143.42us       114  1.2580us     168ns  55.578us  cuDeviceGetAttribute\n",
            "                    0.02%  118.72us         1  118.72us  118.72us  118.72us  cudaGetDeviceProperties\n",
            "                    0.01%  61.164us         4  15.291us  3.8410us  30.366us  cudaEventRecord\n",
            "                    0.01%  53.830us        96     560ns     306ns     788ns  cudaGetLastError\n",
            "                    0.01%  33.613us         4  8.4030us  5.9530us  13.325us  cudaEventSynchronize\n",
            "                    0.00%  12.293us         1  12.293us  12.293us  12.293us  cuDeviceGetName\n",
            "                    0.00%  7.7180us         4  1.9290us  1.0180us  3.4860us  cudaEventDestroy\n",
            "                    0.00%  7.2030us         2  3.6010us  3.1950us  4.0080us  cudaEventElapsedTime\n",
            "                    0.00%  5.5100us         1  5.5100us  5.5100us  5.5100us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.0170us         1  5.0170us  5.0170us  5.0170us  cuDeviceTotalMem\n",
            "                    0.00%  1.7220us         3     574ns     238ns  1.1410us  cuDeviceGetCount\n",
            "                    0.00%  1.3560us         2     678ns     205ns  1.1510us  cuDeviceGet\n",
            "                    0.00%     726ns         1     726ns     726ns     726ns  cuModuleGetLoadingMode\n",
            "                    0.00%     269ns         1     269ns     269ns     269ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW4 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70yJKXNnM5mN"
      },
      "source": [
        "**Câu hỏi**: Giải thích tại sao khi thay đổi kích thước block thì kết quả lại thay đổi như vậy?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JDLuQCYcDZx"
      },
      "source": [
        "**`1.` Quan sát sự thay đổi của kết quả khi thay đổi kích thước block và đưa ra một số nhận xét:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jf10kTHI0XM"
      },
      "source": [
        "Đầu tiên, ta sẽ tạo ra một bảng thống kê thời gian chạy của chương trình ứng với các kích thước block khác nhau. Với mỗi kích thước block, ta chạy chương trình `5 lần`, sau đó tính ra thời gian chạy trung bình. Trước khi bắt đầu ghi nhận thời gian, ta sẽ chạy chương trình một lần để \"làm nóng\" GPU.\n",
        "\n",
        "| Kích thước block (Block size) | Số lượng block (Grid size) | Chạy lần 1 (ms) | Chạy lần 2 (ms) | Chạy lần 3 (ms) | Chạy lần 4 (ms) | Chạy lần 5 (ms) | Thời gian chạy trung bình (ms) |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 256 | 32769 | 940.082 | 945.507 | 907.579 | 917.287 | 940.38 | 930.167 |\n",
        "| 512 | 16385 | 596.125 | 580.446 | 590.943 | 598.832 | 568.071 | 586.883 |\n",
        "| 1024 | 8193 | 424.264 | 439.849 | 397.475 | 420.168 | 430.14 | **422.379** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YDmsYshM5mO"
      },
      "source": [
        "- **Nhận xét**:\n",
        "\n",
        "    - Từ bảng thống kê thời gian chạy của chương trình ứng với các kích thước block khác nhau, ta phát hiện có một xu hướng rõ rệt: *thời gian chạy của chương trình giảm dần khi kích thước block tăng lên*. Điều này được thể hiện rõ qua bảng thống kê, với thời gian chạy trung bình giảm từ 930.167ms (block size 256) xuống còn 422.379ms (block size 1024).\n",
        "\n",
        "    - Ta sẽ tìm hiểu nguyên nhân dẫn đến hiện tượng này bằng cách phân tích chi tiết hơn về các hoạt động của GPU trong khi thực thi chương trình."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBYlqxyycDZy"
      },
      "source": [
        "Quan sát thời gian thực thi các công việc trong phần \"GPU activities\" của `nvprof` ứng với mỗi kích thước block, ta phát hiện rằng thời gian thực thi của các công việc:\n",
        "\n",
        "- Sao chép dữ liệu từ host sang device (HtoD)\n",
        "- Sao chép dữ liệu từ device sang host (DtoH)\n",
        "- Bước rút trích bit\n",
        "- Bước tính rank và ghi kết quả vào mảng dst\n",
        "\n",
        "không thay đổi nhiều khi ta thay đổi kích thước block. Nghĩa là, thời gian thực thi của các công việc này không phụ thuộc quá nhiều vào kích thước block.\n",
        "\n",
        "| Công việc | Thời gian chạy (ms) |\n",
        "| --- | --- |\n",
        "| [CUDA memcpy HtoD] | 15 |\n",
        "| [CUDA memcpy DtoH] | 46 |\n",
        "| extractBitsKernel | 18 |\n",
        "| computeRankAndWriteToDstKernel | 36 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesNN9sfcDZy"
      },
      "source": [
        "Tuy nhiên, thời gian thực thi của bước \"exclusive scan\" để tính ra mảng `nOnesBefore` lại có xu hướng giảm xuống khi ta tăng kích thước block. Và chính sự khác biệt trong thời gian thực thi của hàm kernel này đã ảnh hưởng trực tiếp đến tổng thời gian chạy của chương trình ứng với các kích thước block khác nhau.\n",
        "\n",
        "| Kích thước block (Block size) | Thời gian chạy của bước \"exclusive scan\" (ms) |\n",
        "| --- | --- |\n",
        "| 256 | 809.15 |\n",
        "| 512 | 451.90 |\n",
        "| 1024 | **297.94** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQDk7WLM5mO"
      },
      "source": [
        "- **Nhận xét**:\n",
        "\n",
        "    - Sau khi tiến hành phân tích chi tiết hơn về thời gian thực thi của các công việc có sử dụng GPU, ta nhận thấy rằng nguyên nhân chính dẫn đến sự thay đổi thời gian chạy của chương trình khi thay đổi kích thước block nằm ở bước \"exclusive scan\" để tính mảng `nOnesBefore`. Trong khi thời gian thực thi của các công việc khác như sao chép dữ liệu và các bước kernel khác ít thay đổi, thì thời gian thực thi của bước \"exclusive scan\" giảm mạnh khi kích thước block tăng lên (từ 809.15ms xuống 297.94ms).\n",
        "\n",
        "    - Tiếp theo, ta sẽ phân tích chi tiết về mỗi lần lặp của thuật toán Radix Sort, cũng như cách hoạt động của bước \"exclusive scan\" để hiểu rõ hơn về nguyên nhân dẫn đến sự thay đổi này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cE0Fy1cDZy"
      },
      "source": [
        "**`2.` Phân tích chi tiết về mỗi lần lặp của thuật toán Radix Sort và bước \"exclusive scan\":**\n",
        "\n",
        "Trong mỗi lần lặp (iteration) của thuật toán Radix Sort, ta cần phải thực hiện 3 bước chính:\n",
        "\n",
        "- Bước 1: Rút trích bit thứ `bitIdx` từ mỗi phần tử của mảng `src` và ghi vào mảng `bits`.\n",
        "\n",
        "- Bước 2: Dựa vào mảng `bits`, ta tính ra mảng `nOnesBefore` (số lượng bit 1 trước mỗi phần tử của mảng `src`).\n",
        "\n",
        "- Bước 3: Dựa vào mảng `bits` và `nOnesBefore`, ta sắp xếp lại vị trí của các phần tử trong mảng `src` và ghi kết quả vào mảng `dst`. Vị trí mới của mỗi phần tử trong mảng `src` sẽ được tính dựa vào số lượng bit 0 và bit 1 trước nó (xem công thức tính `rank` trong slide).\n",
        "\n",
        "Cả 3 bước này đều có thể thực hiện song song trên GPU thông qua việc sử dụng các hàm kernel. Trong đó, với \"**bước 1**\" và \"**bước 3**\", các thread block có thể hoạt động độc lập với nhau, tức là các block không cần chờ đợi tín hiệu từ các block khác. Do đó, sự thay đổi về số lượng block cần thiết trong grid không ảnh hưởng nhiều đến hiệu suất của chương trình.\n",
        "\n",
        "Ngược lại, ở \"**bước 2**\", ta cần phải thực hiện một **thuật toán \"exclusive scan\"** song song trên toàn bộ grid, tức là kết quả của mỗi block sẽ có sự phụ thuộc vào kết quả của các block \"trước đó\". Thuật toán \"exclusive scan\" này có thể được thực hiện thông qua 4 bước chính:\n",
        "\n",
        "- Bước 2.1: Tính ra block index $bi$ của mỗi block theo thứ tự được đưa vào SM của chúng.\n",
        "\n",
        "- Bước 2.2: Thực hiện scan cục bộ trên mỗi block để tính ra số lượng bit 1 trước mỗi phần tử trong block đó. Đồng thời, ta cũng cần phải lưu lại tổng số lượng bit 1 của mỗi block vào một mảng riêng để sử dụng cho bước tiếp theo.\n",
        "\n",
        "- Bước 2.3: Block $bi$ sẽ phải chờ đến khi block $bi-1$ báo tín hiệu rằng nó đã hoàn thành việc tính tổng toàn cục của $bi$ block trước đó (từ block 0 đến block $bi-1$). Sau đó, block $bi$ sẽ cộng \"tổng toàn cục của các block trước đó\" vào \"tổng cục bộ của mình\" và truyền tín hiệu cho block $bi+1$ biết rằng nó có thể bắt đầu công việc của mình.\n",
        "\n",
        "- Bước 2.4: Thêm tổng toàn cục của $bi$ block trước đó vào kết quả của mỗi phần tử trong block $bi$.\n",
        "\n",
        "Trong 4 bước chính được liệt kê ở trên, thì chỉ có \"**bước 2.2**\" và \"**bước 2.4**\" là được thực hiện song song một cách \"tự nhiên\", tức là hoạt động của mỗi thread block không phụ thuộc vào (hoặc không cần chờ đợi) các block khác. Còn \"**bước 2.1**\" và \"**bước 2.3**\" lại đòi hỏi sự phối hợp giữa các block, tức là các thread block phải làm việc một cách tuần tự, block $bi$ phải chờ đến khi block $bi-1$ bật tín hiệu \"hoàn thành\" thì mới được thực hiện bước tiếp theo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKgEj28LM5mO"
      },
      "source": [
        "**`3.` Giải thích về ảnh hưởng của kích thước block đến hiệu suất của chương trình:**\n",
        "\n",
        "Lý do cho sự thay đổi về thời gian chạy của chương trình khi thay đổi kích thước block nằm ở cách thuật toán \"exclusive scan\" được thực hiện trên GPU. Như đã phân tích, thuật toán này bao gồm các bước cần sự phối hợp giữa các block, đặc biệt là \"**bước 2.3**\", nơi block $bi$ phải chờ block $bi-1$. **Khi kích thước block tăng lên, thì số lượng block cần thiết để xử lý cùng một lượng dữ liệu giảm xuống**. Điều này dẫn đến:\n",
        "\n",
        "`3.1.` **Giảm số lượng bước đồng bộ hóa giữa các block**:\n",
        "\n",
        "- Khi kích thước block nhỏ, ta sẽ cần nhiều block hơn để xử lý toàn bộ dữ liệu, dẫn đến nhiều lần đồng bộ hóa giữa các block. Điều này gây ra overhead lớn hơn. Khi tăng kích thước block, số lượng block cần thiết để xử lý cùng một khối lượng dữ liệu sẽ giảm, từ đó giảm số lần đồng bộ hóa giữa các block. Đây chính là yếu tố then chốt giúp giảm thời gian thực thi. Việc đồng bộ hóa giữa các block là một hoạt động tốn kém, và việc giảm thiểu nó sẽ cải thiện hiệu suất đáng kể. Điều này giải thích tại sao thời gian thực thi của bước \"exclusive scan\" giảm mạnh khi kích thước block tăng lên.\n",
        "\n",
        "`3.2.` **Tăng hiệu quả sử dụng bộ nhớ chia sẻ (shared memory)**:\n",
        "\n",
        "- Mỗi block sử dụng shared memory để lưu trữ dữ liệu trung gian. Với kích thước block lớn hơn, bộ nhớ chia sẻ có thể được sử dụng hiệu quả hơn vì nhiều dữ liệu có thể được xử lý cục bộ mà không cần truy cập nhiều vào bộ nhớ toàn cục (global memory). Điều này giảm tải băng thông bộ nhớ toàn cục, giúp cải thiện tốc độ truy cập dữ liệu.\n",
        "\n",
        "`3.3.` **Tăng hiệu suất tính toán cục bộ**:\n",
        "\n",
        "- Kích thước block lớn hơn cho phép mỗi block xử lý nhiều dữ liệu hơn ở cục bộ. Điều này có thể tận dụng tốt hơn bộ nhớ shared memory của SM (Streaming Multiprocessor) và giảm thiểu số lần truy cập bộ nhớ global memory, vốn chậm hơn. Bước 2.2 và 2.4 (tính toán cục bộ trong mỗi block) sẽ hiệu quả hơn với block size lớn hơn.\n",
        "\n",
        "**Kết luận**:\n",
        "\n",
        "- Sự giảm thời gian chạy khi tăng kích thước block là do GPU có thể tận dụng tốt hơn khả năng xử lý song song và bộ nhớ chia sẻ, tối ưu hóa việc sử dụng băng thông bộ nhớ.\n",
        "\n",
        "- Đặc biệt, việc tăng kích thước block giúp giảm số lượng block cần thiết, từ đó giảm số lần đồng bộ hóa giữa các block trong bước \"exclusive scan\", dẫn đến giảm đáng kể thời gian thực thi của bước này và do đó giảm tổng thời gian chạy của chương trình.\n",
        "\n",
        "- Điều này cho thấy việc lựa chọn kích thước block phù hợp là rất quan trọng để tối ưu hiệu năng của các thuật toán song song trên GPU, đặc biệt là các thuật toán có bước cần sự phối hợp giữa các block như thuật toán \"exclusive scan\" mà chúng ta đã cài đặt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I7cPRSKcDZy"
      },
      "source": [
        "> References:  \n",
        "> [1] Book \"Programming Massively Parallel Processors: A Hands-on Approach\", chapter \"11 - Prefix sum (scan)\", by David B. Kirk and Wen-mei W. Hwu.  \n",
        "> [2] Book \"Programming Massively Parallel Processors: A Hands-on Approach\", chapter \"13 - Sorting\", by David B. Kirk and Wen-mei W. Hwu.  \n",
        "> [3] Slide bài giảng \"2023-CSC14120-Lecture08-Scan.pdf\" được cung cấp trong môn học.  \n",
        "> [4] Slide bài giảng \"2023-CSC14120-Lecture09-RadixSort.pdf\" được cung cấp trong môn học.  \n",
        "> [5] File code \"08-Scan.cu\" trong phần Demo code trên drive được cung cấp trong môn học.  \n",
        "> [6] \"Chapter 39. Parallel Prefix Sum (Scan) with CUDA\" in \"GPU Gems 3\" by Mark Harris, Shubhabrata Sengupta, John D. Owens - [developer.nvidia.com](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda).  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "graphmining",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
